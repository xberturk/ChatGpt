{
  "model": "{0}",
  "prompt": "{1}",
  "max_tokens": 25,
  "temperature": 0,
  "top_p": 1,
  "n": 1,
  "stream": false,
  "logprobs": null
}


